{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from ipyleaflet import Map, Marker\n",
    "from folium.plugins import MarkerCluster\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import geojson\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from branca.element import Figure\n",
    "import branca.colormap as cm\n",
    "from folium.plugins import TimeSliderChoropleth\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tabulate import tabulate\n",
    "from sqlalchemy import create_engine\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-reaction",
   "metadata": {},
   "source": [
    "# Time Slider for LEA Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_covid_lea_map = gpd.read_file(\"./DCU Data/COVID-19_HPSC_HIU_Timeseries_Local_Electoral_Area_Mapped/COVID-19_HPSC_HIU_Timeseries_Local_Electoral_Area_Mapped.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_covid_lea_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gpd_covid_lea_map[\"FixedDATE\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-louis",
   "metadata": {},
   "source": [
    "### Clean Covid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_connection():\n",
    "    config = {\n",
    "       'host': \"upcom.mysql.database.azure.com\",\n",
    "       'user': \"covid@upcom\",\n",
    "       'password': \"VaL0S8^od\",\n",
    "       'database': \"upcom\",\n",
    "    }\n",
    "    engine = create_engine(\n",
    "       \"mysql+pymysql://\" + config[\"user\"] + \":\" + config[\"password\"] + \"@\" + config[\"host\"] + \"/\" + config[\n",
    "           \"database\"], connect_args={'ssl': {'fake_flag_to_enable_tls': True}})\n",
    "    return engine\n",
    "\n",
    "def get_chunks_df(le_incidence,have_0_index_ls):\n",
    "    lst_df = []\n",
    "    block_df = le_incidence.loc[:have_0_index_ls[0]]\n",
    "    if block_df.shape[0] > 1:\n",
    "        lst_df.append(block_df)\n",
    "    i = 0\n",
    "    while i < len(have_0_index_ls)-1:\n",
    "        block_df = le_incidence.loc[have_0_index_ls[i]:have_0_index_ls[i+1]-1]\n",
    "        lst_df.append(block_df)\n",
    "        i+=1\n",
    "    block_df = le_incidence.loc[have_0_index_ls[-1]:]\n",
    "    if block_df.shape[0] > 0:\n",
    "        lst_df.append(block_df)\n",
    "    return lst_df\n",
    "\n",
    "def cal_p7_upward(le_incidence,start_index,first_p7,max_possi):\n",
    "    le_incidence.loc[start_index - 1, 'P7'] = max_possi - first_p7\n",
    "    temp_up = le_incidence.loc[:start_index].iloc[::-1]\n",
    "    for i in range(1, len(temp_up.index)):\n",
    "        temp_up.loc[temp_up.index.tolist()[1] - i, \"P7\"] = temp_up[\"P14\"].values[i]-temp_up[\"P7\"].values[i]\n",
    "    temp_up = temp_up.iloc[::-1][1:]\n",
    "    return temp_up\n",
    "\n",
    "def cal_p7_downward(le_incidence,max_possi,iszero):\n",
    "    start_index =le_incidence.index[0]\n",
    "    temp_down = le_incidence\n",
    "    for i in range(start_index + 1, temp_down.index[-1] + 1):\n",
    "        if (temp_down.loc[i,\"P14\"]!=0):\n",
    "            temp_down.loc[i, \"P7\"] = temp_down.loc[i, \"P14\"] - temp_down.loc[i - 1, \"P7\"]\n",
    "            if ((temp_down.loc[i, \"P14\"] - temp_down.loc[i - 1, \"P7\"]) < 0):\n",
    "                temp_down.loc[i, \"P7\"] = 0\n",
    "        else:\n",
    "            temp_down.loc[i, \"P7\"] = max_possi - temp_down.loc[i - 1, \"P7\"]\n",
    "    return temp_down\n",
    "\n",
    "def change_zero_two(ls):\n",
    "    chosen_ls = []\n",
    "    for i in range(1,len(ls)-1):\n",
    "        if (ls[i]-ls[i-1]==1) and (ls[i+1]-ls[i]==1):\n",
    "            chosen_ls.append(ls[i])\n",
    "    return chosen_ls\n",
    "\n",
    "def pick_case(le_incidence,lst_possibility,dict_possible,le_no_zero,rightous_df,have_0):\n",
    "    if len(lst_possibility)==1:\n",
    "        rightous_df.append(le_incidence[\"LE_ID\"].unique().tolist()[0])\n",
    "        le_incidence.update(lst_possibility[0])\n",
    "    elif len(lst_possibility)>1:\n",
    "        chosen_one = lst_possibility[0]\n",
    "        dict_possible[le_incidence[\"LE_ID\"].unique().tolist()[0]]=lst_possibility\n",
    "        le_incidence.update(chosen_one)\n",
    "    else:\n",
    "        le_incidence[\"P7\"]=None\n",
    "        le_no_zero.append(le_incidence[\"LE_ID\"].unique().tolist()[0])\n",
    "    return le_incidence\n",
    "\n",
    "def find_appropriate_cases_initial(le_id,have_0,le_incidence,cleaned_county_p7,iszero):\n",
    "    possible_result_ls = []\n",
    "    max_poss=4\n",
    "    # Block of DataFrame\n",
    "    if iszero:\n",
    "        have_0_index_ls = have_0.index.tolist()\n",
    "        chosen_starting = have_0_index_ls[0]\n",
    "    else:\n",
    "        chosen_starting = le_incidence.index.tolist()[0]\n",
    "    df_up = le_incidence.loc[:chosen_starting]\n",
    "    df_down = le_incidence.loc[chosen_starting:]\n",
    "        \n",
    "    for possi_initial_p7 in range(max_poss+1):\n",
    "        df_down.loc[df_down.index[0], \"P7\"] = possi_initial_p7\n",
    "        # Calculate P7_100k downward\n",
    "        temp_down = cal_p7_downward(df_down,max_poss,iszero)\n",
    "        if (temp_down[\"P7\"]>=0).all():\n",
    "            # Calculate P7_100k upward\n",
    "            temp_up = cal_p7_upward(df_up,chosen_starting,possi_initial_p7,max_poss)[:-1]\n",
    "            if (temp_up[\"P7\"]>=0).all():\n",
    "                temp_df = pd.concat([temp_up, temp_down])\n",
    "                possible_result_ls.append(temp_df)\n",
    "    return possible_result_ls\n",
    "\n",
    "def generate_p7_100k(le_id,have_0,le_incidence,dict_possible,le_no_zero,rightous_df,cleaned_county_p7_p14,iszero):\n",
    "    lst_possibility = find_appropriate_cases_initial(le_id,have_0,le_incidence,cleaned_county_p7_p14,iszero)\n",
    "    le_incidence = pick_case(le_incidence,lst_possibility,dict_possible,le_no_zero,rightous_df,have_0)\n",
    "    return le_incidence\n",
    "\n",
    "def get_county_p7(df,days):\n",
    "    df[\"TimeStamp\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    if days == 7:\n",
    "        new_df = df[(df[\"TimeStamp\"]>=datetime.strptime(\"2020/08/02\",'%Y/%m/%d')) & (df[\"TimeStamp\"]<=datetime.strptime(\"2021/11/15\",'%Y/%m/%d')) ].sort_values(by=[\"CountyName\",\"TimeStamp\"])\n",
    "    else:\n",
    "        new_df = df[(df[\"TimeStamp\"]>=datetime.strptime(\"2020/07/26\",'%Y/%m/%d')) & (df[\"TimeStamp\"]<=datetime.strptime(\"2021/11/15\",'%Y/%m/%d')) ].sort_values(by=[\"CountyName\",\"TimeStamp\"])\n",
    "    ls_df = []\n",
    "    for county in new_df[\"CountyName\"].unique().tolist():\n",
    "        county_df = new_df[new_df[\"CountyName\"]==county].reset_index(drop=True)\n",
    "        county_df[\"Actual_New_Cases\"]=county_df[\"ConfirmedCovidCases\"]-county_df[\"ConfirmedCovidCases\"].shift(1)\n",
    "        county_df.drop([0],inplace=True)\n",
    "        county_df.drop(columns=[\"ConfirmedCovidCases\"],inplace=True)\n",
    "        county_df.reset_index(drop=True,inplace=True)\n",
    "        county_df.drop([0], inplace=True)\n",
    "        if days==7:\n",
    "            date_p7 = [county_df.loc[i + 1, \"TimeStamp\"] for i in county_df.index.tolist() if\n",
    "                       (i + 1) % (days) == 0]\n",
    "            p7_cases = [county_df.loc[i-days+1:i][\"Actual_New_Cases\"].sum() for i in county_df.index.tolist() if (i)%days==0]\n",
    "            new_county_df = pd.DataFrame({\"County\":[county for i in range(68)],\"Date\":date_p7,\"P7\":p7_cases})\n",
    "        else:\n",
    "            date_p7 = [county_df.loc[i + 1, \"TimeStamp\"] for i in county_df.index.tolist() if (i + 1) % (days//2) == 0][1:]\n",
    "            p7_cases = [county_df.loc[i-days+1:i][\"Actual_New_Cases\"].sum() for i in county_df.index.tolist() if (i%(days//2)==0)][1:]\n",
    "            new_county_df = pd.DataFrame({\"County\":[county for i in range(68)],\"Date\":date_p7,\"P14\":p7_cases})\n",
    "        ls_df.append(new_county_df)\n",
    "    result = pd.concat(ls_df).reset_index(drop=True)\n",
    "    result[\"County\"] = result[\"County\"].str.upper()\n",
    "    return result\n",
    "\n",
    "def find_correct_p14(row):\n",
    "    accepted_index = [name for name in row.index.tolist() if (\"P7\" in name) & (name!=\"P7_County\")]\n",
    "    needed_row = row[accepted_index]\n",
    "    if (needed_row[needed_row.isnull()].shape[0])==1:\n",
    "        replaced_value = (row[\"P7_County\"]-needed_row[~needed_row.isnull()].sum())//needed_row[needed_row.isnull()].shape[0]\n",
    "        if replaced_value >0:\n",
    "            needed_row[needed_row.isnull()] = replaced_value\n",
    "        else:\n",
    "            needed_row[needed_row.isnull()]= 0\n",
    "        row.update(needed_row)\n",
    "    else:\n",
    "        handling_le_cases = [i[3:] for i in row[row.isnull()].index.tolist()]\n",
    "        for le_id in handling_le_cases:\n",
    "            row[\"P7_\"+le_id] = row[\"P14_\"+le_id]/row[\"P14_County\"]*row[\"P7_County\"]\n",
    "    return row\n",
    "\n",
    "def check_if_exist(le1,le2):\n",
    "    for i in le1:\n",
    "        if i in le2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_right_cases_initial(county_df,county_p7_p14,le_no_zero):\n",
    "    new_index_county = county_df.sort_values(by=[\"LE_ID\",\"FixedDATE\"]).reset_index(drop=True)\n",
    "    sub_lea_ls = [new_index_county.loc[i - 66:i].reset_index(drop=True)[[\"LE_ID\",\"FixedDATE\",\"P14\",\"P7\"]] for i in new_index_county.index.tolist() if (i+1) % 67 == 0]\n",
    "    have_0_df_ls = sorted(set(list(sum([df[df[\"P14\"]==0].index.tolist() for df in sub_lea_ls],[]))))\n",
    "    prv_curr_0_df_ls = sorted(list(set(sum([[i-1,i] for i in have_0_df_ls if (i>0)],[]))))\n",
    "    if have_0_df_ls[0]==0:\n",
    "        prv_curr_0_df_ls=[0]+prv_curr_0_df_ls\n",
    "    lea_df = pd.concat(sub_lea_ls,axis=1)\n",
    "    lea_df[\"Date\"]=lea_df[\"FixedDATE\"].T.drop_duplicates().T\n",
    "    lea_df.drop(columns=[\"FixedDATE\"],inplace=True)\n",
    "    county_incidence = county_p7_p14[county_p7_p14[\"County\"]==county_df[\"COUNTY\"].unique().tolist()[0]]\n",
    "    county_incidence.rename(columns={\"P14\":\"P14_County\",\"P7\":\"P7_County\"},inplace=True)\n",
    "    col_ls = lea_df.columns.tolist()\n",
    "    for index,col in enumerate(col_ls):\n",
    "        if col==\"P14\":\n",
    "            col_ls[index] = col+\"_\"+str(float(lea_df.iloc[:,(index-1)].unique()[0]))\n",
    "        elif col==\"P7\":\n",
    "            col_ls[index] = col + \"_\" + str(float(lea_df.iloc[:, (index - 2)].unique()[0]))\n",
    "    lea_df.columns = col_ls\n",
    "    le_ls = county_df[\"LE_ID\"].unique().tolist()\n",
    "    county_incidence[\"Date\"]=county_incidence[\"Date\"].astype(str)\n",
    "    merged_county_lea = lea_df.merge(county_incidence.set_index(\"Date\"), left_on=\"Date\", right_index=True)\n",
    "    if (check_if_exist(le_ls,le_no_zero)):\n",
    "        lea_df.drop(columns=[\"LE_ID\"],inplace=True)\n",
    "        merged_county_lea[\"Current Total\"]=merged_county_lea[[col for col in merged_county_lea.columns if (\"P7\" in col) and (\"County\" not in col)]].sum(axis=1)\n",
    "        p14ls = [col for col in merged_county_lea.columns.tolist() if (\"P7\" in col) | (\"P14\" in col)]\n",
    "        p14_df = merged_county_lea[p14ls]\n",
    "        p14_df = p14_df.apply(lambda x: find_correct_p14(x),axis=1)\n",
    "        merged_county_lea.update(p14_df)\n",
    "        county_df.update(update_initial_record(merged_county_lea,county_df,le_no_zero))\n",
    "    return county_df\n",
    "\n",
    "def update_initial_record(merged_county_lea,reorder_incidence,le_no_zero):\n",
    "    le_ls = [float(i) for i in merged_county_lea[\"LE_ID\"].drop_duplicates().values.tolist()[0]]\n",
    "    replaced_le_id = list(set(le_no_zero).intersection(set(le_ls)))\n",
    "    temp_merged_lea = merged_county_lea.drop(columns=[\"LE_ID\"])\n",
    "    none_county = reorder_incidence[reorder_incidence[\"P7\"].isnull()&reorder_incidence[\"LE_ID\"].isin(replaced_le_id)]\n",
    "    for le_id in replaced_le_id:\n",
    "        temp_le_df = none_county[none_county[\"LE_ID\"]==le_id]\n",
    "        temp_le_df[\"P7\"] = temp_merged_lea[\"P7_\"+str(float(le_id))].values\n",
    "        none_county.update(temp_le_df)\n",
    "    reorder_incidence.update(none_county)\n",
    "    return reorder_incidence\n",
    "\n",
    "def find_p7_ireland(gpd_covid_lea_map):\n",
    "    engine = db_connection()    \n",
    "    incidence_df = pd.DataFrame(gpd_covid_lea_map.drop(columns=[\"geometry\",\"FID\",\"DATE\",\"GUID\"]))\n",
    "    demo_df = pd.read_sql(\"SELECT LE_ID,total_population_age FROM upcom.lea_details\",con=engine).drop_duplicates()\n",
    "    demo_df[\"LE_ID\"] = demo_df[\"LE_ID\"].astype(str)\n",
    "    incidence_df = pd.merge(incidence_df,demo_df,on=\"LE_ID\")\n",
    "    reorder_incidence = incidence_df.sort_values(by=[\"LE_ID\", \"FixedDATE\"]).reset_index(drop=True)\n",
    "    reorder_incidence[\"P14\"]=reorder_incidence[\"Rate100K\"]*reorder_incidence[\"total_population_age\"]/100000\n",
    "    reorder_incidence[\"P14\"] = round(reorder_incidence[\"P14\"])\n",
    "    reorder_incidence[\"P7\"] = None\n",
    "    reorder_incidence[\"P7_100k\"] = None\n",
    "    le_no_zero = []\n",
    "    dict_possible= {}\n",
    "    rightous_df_ls = []\n",
    "    le_lst = reorder_incidence[\"LE_ID\"].unique().tolist()\n",
    "    county_cases = pd.read_csv(\"./DCU Data/COVID-19_HPSC_County_Statistics_Historic_Data.csv\",usecols=[\"CountyName\",\"ConfirmedCovidCases\",\"TimeStamp\"])\n",
    "    county_cases[\"TimeStamp\"]=county_cases[\"TimeStamp\"].str.split(\" \").str[0]\n",
    "    cleaned_county_p7 = get_county_p7(county_cases,7)\n",
    "    cleaned_county_p14 = get_county_p7(county_cases,14)\n",
    "    cleaned_county_p7_14 = pd.concat([cleaned_county_p7,cleaned_county_p14[[\"P14\"]]],axis=1)\n",
    "\n",
    "\n",
    "    # Find possible P7\n",
    "    for index in tqdm(range(len(le_lst))):\n",
    "        le_id = le_lst[index]\n",
    "        le_incidence = reorder_incidence[reorder_incidence[\"LE_ID\"] == le_id]\n",
    "        have_0 = le_incidence[le_incidence[\"Rate100K\"] == 0]\n",
    "        iszero=True\n",
    "        if (have_0.shape[0] == 0):\n",
    "            iszero=False\n",
    "        le_incidence = generate_p7_100k(le_id,have_0,le_incidence,dict_possible,le_no_zero,rightous_df_ls,cleaned_county_p7_14,iszero)\n",
    "        reorder_incidence.update(le_incidence)\n",
    "\n",
    "    reorder_incidence[\"P7_100k\"]=reorder_incidence[\"P7\"]*100000/reorder_incidence[\"total_population_age\"]\n",
    "    return reorder_incidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-greensboro",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_data = find_p7_ireland(gpd_covid_lea_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-network",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tabulate(covid_data.head(), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-muscle",
   "metadata": {},
   "source": [
    "### Clean Covid map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join cleaned COvid data with Geometry\n",
    "gpd_covid_lea_map[[\"P14\",\"P7\",\"P7_100k\"]] = covid_data[[\"P14\",\"P7\",\"P7_100k\"]]\n",
    "# Prepare data\n",
    "max_colour = max(gpd_covid_lea_map['Rate100K'])\n",
    "min_colour = sorted(gpd_covid_lea_map['Rate100K'].unique().tolist())[1]\n",
    "defined_scale = [i for i in range(int(min_colour), int(max_colour)+int(max_colour//12), int(max_colour//12))]\n",
    "cmap = cm.linear.RdYlBu_11.to_step(12,index=defined_scale)\n",
    "cmap.caption = \"Number of COVID cases by LEAs every week\"\n",
    "gpd_covid_lea_map['colour'] = gpd_covid_lea_map['Rate100K'].map(cmap)\n",
    "gpd_covid_lea_map_only_0 = gpd_covid_lea_map[gpd_covid_lea_map[\"Rate100K\"]==0]\n",
    "gpd_covid_lea_map_only_0[\"colour\"]=\"\"\n",
    "gpd_covid_lea_map.update(gpd_covid_lea_map_only_0)\n",
    "gpd_covid_lea_map[\"DATE\"]=(pd.DatetimeIndex(gpd_covid_lea_map['FixedDATE']).astype(int)// 10 ** 9).astype('U10')\n",
    "temp_gpd = gpd_covid_lea_map[gpd_covid_lea_map[\"colour\"]==\"\"]\n",
    "temp_gpd[\"colour\"]=\"#00FFFFFF\"\n",
    "gpd_covid_lea_map.update(temp_gpd)\n",
    "short_gpd = gpd_covid_lea_map[[\"LE_ID\",\"geometry\"]].drop_duplicates().reset_index()\n",
    "short_gpd.drop(columns=[\"index\"],inplace=True)\n",
    "lea_list = short_gpd['LE_ID'].unique().tolist()\n",
    "lea_idx = range(len(lea_list))\n",
    "style_dict = {}\n",
    "for i in lea_idx:\n",
    "    lea = lea_list[i]\n",
    "    result = gpd_covid_lea_map[gpd_covid_lea_map['LE_ID'] == lea]\n",
    "    inner_dict = {}\n",
    "    for _, r in result.iterrows():\n",
    "        inner_dict[r['DATE']] = {'color': r['colour'], 'opacity': 1}\n",
    "    style_dict[i] = inner_dict\n",
    "\n",
    "new_lea = gpd_covid_lea_map.drop(columns=[\"Rate100K\",\"DATE\"]).drop_duplicates()\n",
    "new_lea = new_lea.reset_index().drop(columns=[\"index\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-title",
   "metadata": {},
   "source": [
    "### Generate Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Map\n",
    "fig6=Figure(height=850,width=1000)\n",
    "center = [53.52876390714696, -8.070723942287891]\n",
    "bounds = [[51.381808536222344, -4.9184898320881025],[55.45217379512414, -11.451803787292237]]\n",
    "slider_map = folium.Map(location = center, zoom_start=7, tiles='cartodbpositron',min_zoom = 7,maxBounds= bounds,\n",
    "  maxBoundsViscosity= 1)\n",
    "\n",
    "\n",
    "# lea_layer = folium.FeatureGroup(name=\"LEAs\",show=True).add_to(slider_map)\n",
    "# style2 = {'fillColor': '#00FFFFFF', 'color': '#00FFFFFF'}\n",
    "# lea_layer.add_child(folium.GeoJson(data=short_gpd,popup=folium.GeoJsonPopup(fields=[\"LE_ID\"]),style_function=lambda x:style2)).add_to(slider_map)\n",
    "\n",
    "\n",
    "\n",
    "fig6.add_child(slider_map)\n",
    "g = TimeSliderChoropleth(\n",
    "    short_gpd.to_json(),\n",
    "    styledict=style_dict,\n",
    ").add_to(slider_map)\n",
    "g = cmap.add_to(slider_map)\n",
    "slider_map.save(\"TimeSlice.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-curve",
   "metadata": {},
   "source": [
    "# Road network and traffic counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_network = gpd.read_file(\"./DCU Data/Roads_-_OSi_National_250k_Map_of_Ireland/Roads_-_OSi_National_250k_Map_of_Ireland.shp\")\n",
    "traffic_location_df = pd.read_csv(\"./DCU Data/tmu-sites.tsv\",sep='\\t',header=None)\n",
    "traffic_count_df = pd.read_csv(\"traffic_count_week.csv\")\n",
    "traffic_count_df[\"siteId\"] = traffic_count_df[\"siteId\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in traffic_count_df[\"siteId\"].unique():\n",
    "#     print(tabulate(traffic_count_df.head(100), headers='keys', tablefmt='psql'))\n",
    "    print (len((traffic_count_df[traffic_count_df[\"siteId\"]==i][\"Time\"].unique().tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-somalia",
   "metadata": {},
   "source": [
    "### Clean Traffic Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean traffic_location_df\n",
    "# Generate map\n",
    "def generate_map(map_mpp,df,column_to_show,color,marker_cluster):\n",
    "    type_data = \"Traffic Counter\"\n",
    "    show_type=True\n",
    "    locations = df[['Lat', 'Lon']]\n",
    "    locationlist = locations.values.tolist()\n",
    "    for point in range(0, len(locationlist)):\n",
    "        folium.Marker(locationlist[point], popup=folium.Popup(folium.IFrame(\"<b>Name:</b> \"+df[column_to_show][point]+'<br><b>Type:</b> '+type_data,width=250,height=80),max_width=250), icon = folium.Icon(color=color)).add_to(marker_cluster)\n",
    "   \n",
    "    return map_mpp\n",
    "\n",
    "# Convert Lat Lon columns to Geometry column\n",
    "def generate_geometry(df,lat,lon):\n",
    "    geometry = [Point(xy) for xy in zip(df[lon], df[lat])]\n",
    "    df = gpd.GeoDataFrame(df, crs='EPSG:4326', geometry=geometry)\n",
    "    return df\n",
    "\n",
    "traffic_location_df.columns = [\"NRA\",\"siteId\",\"Direction\",\"Location\",\"Lat\",\"Lon\",\"Nothing\"]\n",
    "traffic_location_df.drop(columns=[\"Nothing\",\"NRA\"],inplace=True)\n",
    "traffic_location_df = traffic_location_df[~traffic_location_df[\"Location\"].str.lower().str.contains(\"test\")]\n",
    "traffic_location_df.dropna(inplace=True)\n",
    "traffic_location_df.reset_index(inplace=True,drop=True)\n",
    "traffic_location_df[\"Lat\"]=traffic_location_df[\"Lat\"].astype(float)\n",
    "traffic_location_df[\"Lon\"]=traffic_location_df[\"Lon\"].astype(float)\n",
    "traffic_location_df = generate_geometry(traffic_location_df,\"Lat\",\"Lon\")\n",
    "traffic_location_df = traffic_location_df.drop_duplicates(subset=[\"geometry\"])\n",
    "traffic_location_df.reset_index(drop=True,inplace=True)\n",
    "traffic_location_df[\"siteId\"] =traffic_location_df[\"siteId\"].str.strip(\"0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tabulate(traffic_location_df, headers='keys', tablefmt='psql'))\n",
    "# type(traffic_location_df[\"Lat\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-liberal",
   "metadata": {},
   "source": [
    "### Clean Road Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closet_traffic_counter(row,gpd_network):\n",
    "    gpd_network[\"Distance\"]=gpd_network[\"geometry\"].apply(lambda x: x.distance(row))\n",
    "    min_id = gpd_network[gpd_network[\"Distance\"]==min(gpd_network[\"Distance\"].unique().tolist())][\"OBJECTID\"].tolist()\n",
    "    return min_id[0]\n",
    "traffic_location_df[\"Road_ID\"]=traffic_location_df[\"geometry\"].apply(lambda x: find_closet_traffic_counter(x,gpd_network))\n",
    "gpd_network.drop(columns=[\"Distance\"],inplace=True)\n",
    "gpd_network.reset_index(drop=True,inplace=True)\n",
    "selected_route = gpd_network[gpd_network[\"OBJECTID\"].isin(traffic_location_df[\"Road_ID\"].unique())].reset_index(drop=True)\n",
    "\n",
    "traffic_location_df = traffic_location_df.merge(traffic_count_df,how=\"inner\",on=\"siteId\")\n",
    "traffic_location_df['colour'] = traffic_location_df['traffic_count'].map(cmap)\n",
    "# traffic_location_df = pd.merge(traffic_location_df,selected_route,how=\"inner\",left_on=\"Road_ID\",right_on=\"OBJECTID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-landscape",
   "metadata": {},
   "source": [
    "### Generate Map with Folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-argentina",
   "metadata": {},
   "source": [
    "### Slider for Linestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = traffic_location_df.drop(columns=[\"Direction\",\"Lat\",\"Lon\",\"siteId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_dict2 = {}\n",
    "# road_ID_list = temp['Road_ID'].unique().tolist()\n",
    "# temp[\"DATE\"]=(pd.DatetimeIndex(temp['Time']).astype(int)// 10 ** 9).astype('U10')\n",
    "# road_idx = range(len(road_ID_list))\n",
    "# for i in road_idx:\n",
    "#     road = road_ID_list[i]\n",
    "#     result = temp[temp['Road_ID'] == road]\n",
    "#     inner_dict = {}\n",
    "#     for _, r in result.iterrows():\n",
    "#         inner_dict[r['DATE']] = {'color': r['colour'], 'opacity': 1}\n",
    "#     style_dict2[i] = inner_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geojson_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type':'Point', \n",
    "                'coordinates':[row['Lon'],row['Lat']]\n",
    "            },\n",
    "            'properties': {\n",
    "                'time': row['Time'],\n",
    "                'style': {'color' : row['colour']},\n",
    "                'icon': 'circle',\n",
    "                'iconstyle':{\n",
    "                    'fillColor': row['colour'],\n",
    "                    'fillOpacity': 0.8,\n",
    "                    'stroke': 'true',\n",
    "                    'radius': 4\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_map(features):\n",
    "    center = [53.52876390714696, -8.070723942287891]\n",
    "    bounds = [[51.381808536222344, -4.9184898320881025],[55.45217379512414, -11.451803787292237]]\n",
    "    road_count_map = folium.Map(location = center, zoom_start=7, tiles='cartodbpositron',min_zoom = 7, maxBounds= bounds,\n",
    "      maxBoundsViscosity= 1,control_scale=True)\n",
    "    \n",
    "    TimestampedGeoJson(\n",
    "        {'type': 'FeatureCollection',\n",
    "        'features': features}\n",
    "        , period='P7D'\n",
    "        , add_last_point=True\n",
    "        , auto_play=False\n",
    "        , loop=False\n",
    "        , max_speed=1\n",
    "        , transition_time=1\n",
    "        , loop_button=True\n",
    "        , date_options='DD/MM/YYYY'\n",
    "        , time_slider_drag_update=True\n",
    "    ).add_to(road_count_map)\n",
    "    return road_count_map\n",
    "\n",
    "def plot_pollutant(df):\n",
    "    features = create_geojson_features(df)\n",
    "    return make_map(features), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "road_count_map, df = plot_pollutant(traffic_location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_colour2 = max(traffic_location_df['traffic_count'])\n",
    "min_colour2 = sorted(traffic_location_df['traffic_count'].unique().tolist())[1]\n",
    "defined_scale2 = [i for i in range(int(min_colour), int(max_colour2)+int(max_colour2//12), int(max_colour2//12))]\n",
    "cmap2 = cm.linear.RdYlBu_11.to_step(12,index=defined_scale2)\n",
    "cmap2.caption = \"Average numbers of movement every week\"\n",
    "road_count_map.add_child(cmap2)\n",
    "\n",
    "\n",
    "lea_layer = folium.FeatureGroup(name=\"LEAs\",show=True).add_to(road_count_map)\n",
    "style2 = {'color': '#00FFFFFF'}\n",
    "lea_layer.add_child(folium.GeoJson(data=short_gpd,popup=folium.GeoJsonPopup(fields=[\"LE_ID\"]),style_function=lambda x:style2)).add_to(road_count_map)\n",
    "\n",
    "\n",
    "lea_layer = folium.FeatureGroup(name=\"Road Network\",show=True).add_to(road_count_map)\n",
    "lea_layer.add_child(folium.GeoJson(data=selected_route)).add_to(road_count_map)\n",
    "\n",
    "\n",
    "folium.LayerControl(position=\"bottomright\").add_to(road_count_map)\n",
    "\n",
    "road_count_map.fit_bounds(center) \n",
    "road_count_map.save(\"RoadNetwork\"+\".html\")\n",
    "# road_count_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-anger",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "htmlmap = HTML('<iframe srcdoc=\"{}\" style=\"float:left; width: {}px; height: {}px; display:inline-block; width: 50%; margin: 0 auto; border: 2px solid black\"></iframe>'\n",
    "           '<iframe srcdoc=\"{}\" style=\"float:right; width: {}px; height: {}px; display:inline-block; width: 50%; margin: 0 auto; border: 2px solid black\"></iframe>'\n",
    "           .format(slider_map.get_root().render().replace('\"', '&quot;'),1000,1000,\n",
    "                   road_count_map.get_root().render().replace('\"', '&quot;'),1000,1000))\n",
    "display(htmlmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-congo",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
